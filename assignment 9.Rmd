---
title: "assignment 9"
author: "Yong Qiao"
date: "11/14/2020"
output: pdf_document
---
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\bx}{\textbf{x}}
\newcommand{\bX}{\textbf{X}}
\newcommand{\bz}{\textbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercises

1. Suppose $X$ has the following probability density function
\[
f(x) = \frac{1}{5\sqrt{2\pi}}x^2 e^{-\frac{(x-2)^2}{2}}, \qquad -\infty <x < \infty.
\]
Consider using the importance sampling method to estimate $\E(X^2)$.
    a. Implement the important sampling method, with $g(x)$ being the standard
normal density. Report your estimates using 1000, 10000 and 50000
samples. Also estimate the variances of the estimates.
    a. Design a better importance sampling method to estimate $\E(X^2)$ using a
different $g(x)$. Justify your choice of $g(x)$.
    a. Implement your method and estimate $\E(X^2)$ using using 1000, 10000 and 50000
samples. Also estimate the variances of the importance sampling estimates.
    a. Compare the two results from the two methods and comment.
    
### Solution:
$$\begin{aligned}
E(X^2) &= \int^{+\infty}_{-\infty} x^2 f(x)dx\\
&= \int^{+\infty}_{-\infty} x^2 \frac{1}{5 \sqrt{2 \pi}} x^2 e^{-\frac{(x-2)^2}{2}} dx\\
&= \int^{+\infty}_{-\infty} x^2 \frac{1}{5} x^2 e^{-(2-2x)} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx\\    
&= \int^{+\infty}_{-\infty} h(x) \frac{f(x)}{g(x)} g(x) dx
\end{aligned}$$
we have a standard normal density, and we have 
$w(x) = \frac{f(x)}{g(x)} = \frac{1}{5} x^2 e^{-(2-2x)}$

```{r}
w <- function(x) {
  x^2 * exp(2 * x - 2) / 5
  }
sim <- function(n) {
  est <- c()
  for(i in 1:1e3) {
    x <- rnorm(n)
    est <- c(est, mean(x^2 * w(x)))
    }
  return(c(mean = mean(est), variance = sd(est)^2))
  }

sim(1000)

sim(10000)

sim(50000)
```
To reduce the variance of the estimate, $g(x)$ should be in proportion to $h(x)f(x)$ as much as possible. So  $g(x)$ could be $N(2, 1)$ to reduce the variance of our estimate. so $w(x) = \frac{f(x)}{g(x)} = \frac{x^2}{5}$
```{r}
w <- function(x) {
  x^2 / 5
  }
sim <- function(n) {
  est <- c()
  for(i in 1:1e3) {
    x <- rnorm(n, 2, 1)
    est <- c(est, mean(x^2 * w(x)))
    }
  return(c(mean = mean(est), variance = sd(est)^2))
  }

sim(1000)

sim(10000)

sim(50000)
```
From the result we can see that the variance are decrease for both method when the sample size increase and when same size is the same the later mathod is more stable and have less variance.

1. Consider a geometric Brownian motion
\begin{align*}
  \frac{\dd S(t)}{S(t)} = r\,\dd t + \sigma\,\dd W(t).
\end{align*}
Let $P_A = e^{-rT}(S_A-K)_+$, $P_E = e^{-rT}[S(T)-K]_+$, and $P_G =
e^{-rT}[S_G-K]_+$, where
\begin{align*}
  S_A = \frac{1}{n} \sum_{i=1}^n S\left(\frac{iT}{n}\right),
  \quad
  S_G = \left[\prod_{i=1}^n S\left(\frac{iT}{n}\right)\right]^{1/n}.
\end{align*}
In all the questions below, $S(0)=1$, $r=0.05$, and $n=12$.
    a. Write down and implement an algorithm to sample the path of $S(t)$.
    a. Set $\sigma=0.5$ and $T=1$.  For each of the values of 
$K \in \{1.1, 1.2, 1.3, 1.4, 1.5\}$, simulate 5000 sample paths of $S(t)$ to get
MC estimates of the correlation coefficients between $P_A$ and
$S(T)$, between $P_A$ and $P_E$, and between $P_A$ and $P_G$.  How
do the correlation coefficients change as $K$ increases?
    a. Set $T=1$ and $K=1.5$.  For each of the values of 
$\sigma \in \{0.2, 0.3, 0.4, 0.5\}$, simulate 5000 sample paths of $S(t)$ to
get MC estimates of the correlation coefficients.  How do the correlation
coefficients change as $\sigma$ increases?
    a. Set $\sigma=0.5$ and $K=1.5$.  For each of the values of 
$T \in \{0.4, 0.7, 1, 1.3, 1.6\}$, use 5000 sample paths of $S(t)$ to get MC
estimates of the correlation coefficients.  How do the correlation
coefficients change as $T$ increases?
    a. Set $\sigma=0.4$, $T=1$ and $K=1.5$.  Use $P_G$ as a control
variate to develop a control variate MC estimator for $\E(P_A)$.
Compare its SD with the SD of the MC estimator for $\E(P_A)$ that
has no control variate.
### Solution
$$S(T) =  S(0) e^{(r-\frac{1}{2}\sigma^2)T + \sigma W(T)},$$
$W(T) =  \sqrt{T}Z$, $Z \sim N(0,1)$.
```{r}
set.seed(234)
rBM <- function(n, tgrid, sigma, r, S0) {
  tt <- c(0, tgrid)
  dt <- diff(tt)
  nt <- length(tgrid)
  dw <- matrix(rnorm(n * nt, sd = sigma * sqrt(dt)), n, nt, byrow = TRUE)
  wt = t(apply(dw, 1, cumsum))
  St <- S0 * exp((r - sigma^2 / 2) * matrix(tgrid, n, nt, byrow = TRUE) + wt)
  return(St)
}
tgrid <-  seq(0, 1, length = 12)[-1]; sigma <- 0.5; S0=1; r=0.05; n = 1 
plot(drop(rBM(n, tgrid, sigma, r, S0)), ylab = 'Sample Path of S(t)')
plot(drop(rBM(n, tgrid, sigma, r, S0)), ylab = 'Sample Path of S(t)')

```

```{r}
optValueAppr <- function(n, r, sigma, S0, K, tgrid) {
  nt <- length(tgrid)
  TT <- tgrid[nt]
  St <- rBM(n, tgrid, sigma, r, S0)
  pAri <- pmax(rowMeans(St) - K, 0) * exp(-r * TT)
  ST <- St[, nt]
  pStd <- pmax(ST - K, 0) * exp(-r * TT)
  pGeo <- pmax(exp(rowMeans(log(St))) - K, 0) * exp(-r * TT)
  list(pAri = pAri, ST = ST, pStd = pStd, pGeo = pGeo )
}
n = 5000;
r = 0.05; 
sigma = 0.5;
S0 = 1; 
tgrid = seq(0, 1, length = 12)[-1]
k<-c(1.1,1.2,1.3,1.4,1.5)
#K    cor(P(A),S(T))   cor(P(A),P(E))   cor(P(A),P(G)) 
for (i in 1:5) {
 res<-optValueAppr(n, r, sigma, S0, K =k[i], tgrid)
 print(c(k[i],cor(res$pAri,res$ST),cor(res$pAri,res$pStd),cor(res$pAri,res$pGeo)))
 }

```
As K increases, correlation coefficients decreases.

```{r}
n = 5000;
r = 0.05; S0 = 1;
K=1.5; 
tgrid = seq(0, 1, length = 12)[-1]
sigma<-c(0.2,0.3,0.4,0.5)
#sigma    cor(P(A),S(T))   cor(P(A),P(E))   cor(P(A),P(G)) 
for (i in 1:4) {
 res<-optValueAppr(n, r, sigma[i], S0, K , tgrid)
 print(c(sigma[i],cor(res$pAri,res$ST),cor(res$pAri,res$pStd),cor(res$pAri,res$pGeo)))
 }


```
As sigma increases, correlation coefficients decreases.
```{r}
t<-c(0.4,0.7,1,1.3,1.6)
n = 5000; 
r = 0.05; 
sigma = 0.5; 
S0 = 1; 
K= 1.5
#t   cor(P(A),S(T))   cor(P(A),P(E))   cor(P(A),P(G)) 

for (i in 1:5) {
  res= optValueAppr(n, r, sigma, S0, K, tgrid = seq(0, t[i], length = 12)[-1])
  print(c(t[i],cor(res$pAri,res$ST),cor(res$pAri,res$pStd),cor(res$pAri,res$pGeo)))
}

```
As T increases, correlation coefficients increases.
```{r}

callValLognorm <- function(S0, K, mu, sigma) {
  d <- (log(S0 / K) + mu + sigma^2) / sigma
  S0 * exp(mu + 0.5 * sigma^2) * pnorm(d) - K * pnorm(d - sigma)
}
optValueAppr <- function(n, r = 0.05, sigma = 0.3, S0 = 50, K = 50,tgrid = seq(0, .25, length = 14)[-1]){
  nt <- length(tgrid)
  TT <- tgrid[nt]
  St <- rBM(n, tgrid, sigma, r, S0)
  pAri <- pmax(rowMeans(St) - K, 0) * exp(-r * TT)
  vAri <- mean(pAri) 
  ST <- St[, nt]
  pStd <- pmax(ST - K, 0) * exp(-r * TT)
  pGeo <- pmax(exp(rowMeans(log(St))) - K, 0) * exp(-r * TT)
  pAri <- pmax(rowMeans(St) - K, 0)
  tbar <- mean(tgrid)
  sBar2 <- sigma^2 / nt^2 / tbar * sum( (2 * seq(nt) - 1) * rev(tgrid) )
  pGeoTrue <- callValLognorm(S0, K, (r - 0.5 * sigma^2) * tbar,sqrt(sBar2 * tbar))
  vGeo <- vAri - cov(pGeo, pAri) / var(pGeo) * (mean(pGeo) - pGeoTrue * exp(-r * TT) )
  c(vGeo, vAri)
}
sim <- replicate(1000, optValueAppr(n, r = 0.05, sigma = 0.4, S0 = 1, K = 1.5, 
                                    tgrid = seq(0,1,length = 12)[-1]))
apply(sim, 1, mean)
apply(sim, 1, sd)
```
if we control MC the estimate has smaller variance. 
